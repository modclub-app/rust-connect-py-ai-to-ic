# multi_query_single_canister_demo



## Usage

Once the setup is complete, you can proceed with the following steps to build, deploy, and run your project.

1. Build the Rust project targeting WebAssembly:
   ```bash
   cargo build --target wasm32-unknown-unknown --release -p single_query_demo_backend
   ```
2. Start the Dfinity network locally in the background:
   ```bash
   dfx start --background
   ```
3. Deploy your project using `dfx`:
   ```bash
   dfx deploy
   ```
4. Use the Cargo command to run specific tasks, such as uploading model chunks. Replace the demo models `[gpt2_embedding.onnx, gpt2_layer_0.onnx]` with your actual model file names:
   ```bash
   cargo run --manifest-path ../../rust/upload_byte_file/Cargo.toml demo_gpt2_model_backend upload_model_chunks ../../python/onnx_model/ [gpt2_embedding.onnx, gpt2_layer_0.onnx] 0
   ```

### Demo

Follow these steps to run a demo of the application. This demo involves using the Python transformers library, executing a Python script for model partitioning, and interacting with the backend Canister API.

1. **Install Python Transformers Library**  
   This project uses the transformers library for handling models. Install it using pip:
   ```bash
   pip install transformers
   ```

2. **Execute the Python Script for Model Partitioning**  
   Run the provided Python script to partition the GPT-2 model. This script prepares the model for the backend.
   ```bash
   python3 python/GPT2_max_partition_model_pool.py
   ```

3. **Execute Model Loading Commands**  
   Once the model is partitioned, load the model chunks into the backend using the Cargo command mentioned in the Usage section. Navigate to the canister scripts (internet_computer/demo_gpt2_model) and initialize a local network and deploy the canister. Then, upload the model files generated by the Python script:
   ```bash
   cargo run --manifest-path ../../rust/upload_byte_file/Cargo.toml single_query_demo_backend upload_model_chunks ../../python/onnx_model/ [gpt2_embedding.onnx, gpt2_layer_0.onnx] 0
   ```
   Or for uploading onto the internet computer mainnet
   ```bash
   cargo run --manifest-path ../../rust/upload_byte_file/Cargo.toml <canister id>  upload_model_chunks ../../python/onnx_model/ [gpt2_embedding.onnx, gpt2_layer_0.onnx] 0 ic
   ```

model_plan_to_plan: () → ()
plan_to_running_model: () → ()

If the upload gets interupted call
"wasm_ref_cell_length": () -> (nat64) query;

and take that result and use it when reloading
   cargo run --manifest-path ../../rust/upload_byte_file/Cargo.toml single_query_demo_backend upload_model_chunks ../../python/onnx_model/ [gpt2_embedding.onnx] <result number>

4. **Interact with the Backend Canister API**  
   Navigate to the backend Canister API. The specific endpoint to use is `word_embeddings: (vec int64) → (vec float32) composite_query`.
   
5. **Input Tokens for Testing**  
   Test the API by inputting tokens in the format `[1, 4, 5]`. This will demonstrate the model's ability to generate word embeddings based on the input tokens.

